#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script pour inspecter les donnÃ©es SQL dump stockÃ©es en base
Affiche le contenu exact des tables DataFrameData pour diagnostiquer le problÃ¨me
"""

import json
import sys
import os
from pathlib import Path

def inspect_sql_dump_data():
    """Inspecte les donnÃ©es SQL dump en base"""
    print("ğŸ” === INSPECTION DONNÃ‰ES SQL DUMP ===")
    
    # Ajouter le rÃ©pertoire backend au PYTHONPATH
    backend_dir = Path("backend")
    if backend_dir.exists():
        sys.path.insert(0, str(backend_dir))
    
    try:
        # Import des modules backend
        from app.db.session import SessionLocal
        from app.models.project import DataSource, DataFrameData
        
        # Connexion Ã  la base
        db = SessionLocal()
        
        try:
            # Chercher les sources SQL dump
            sql_sources = db.query(DataSource).filter(DataSource.type == 'sql').all()
            
            if not sql_sources:
                print("âŒ Aucune source de donnÃ©es SQL dump trouvÃ©e en base")
                return
            
            print(f"ğŸ“„ {len(sql_sources)} source(s) SQL dump trouvÃ©e(s):")
            
            for source in sql_sources:
                print(f"\n{'='*60}")
                print(f"ğŸ“Š SOURCE: {source.name} (ID: {source.id})")
                print(f"   - Fichier: {source.file_path}")
                print(f"   - Type: {source.type}")
                print(f"   - Actif: {source.is_active}")
                print(f"   - CrÃ©Ã©: {source.created_at}")
                print(f"   - Mis Ã  jour: {source.updated_at}")
                
                # VÃ©rifier le schÃ©ma
                if source.schema_info:
                    try:
                        schema = json.loads(source.schema_info)
                        print(f"\nğŸ“‹ SCHÃ‰MA:")
                        print(json.dumps(schema, indent=2, ensure_ascii=False))
                    except json.JSONDecodeError:
                        print(f"âŒ SchÃ©ma invalide: {source.schema_info}")
                else:
                    print(f"âŒ Aucun schÃ©ma enregistrÃ©")
                
                # Compter les donnÃ©es
                data_count = db.query(DataFrameData).filter(DataFrameData.data_source_id == source.id).count()
                print(f"\nğŸ“Š DONNÃ‰ES EN BASE: {data_count} lignes")
                
                if data_count == 0:
                    print(f"âŒ Aucune donnÃ©e stockÃ©e pour cette source!")
                    continue
                
                # RÃ©cupÃ©rer toutes les donnÃ©es pour inspection
                all_data = db.query(DataFrameData).filter(
                    DataFrameData.data_source_id == source.id
                ).order_by(DataFrameData.row_index).all()
                
                print(f"\nğŸ“‹ CONTENU COMPLET DES DONNÃ‰ES:")
                print(f"{'='*60}")
                
                for i, row in enumerate(all_data):
                    try:
                        data = json.loads(row.row_data)
                        print(f"\nLigne {row.row_index}:")
                        for key, value in data.items():
                            print(f"  {key}: {value}")
                    except json.JSONDecodeError as e:
                        print(f"âŒ Erreur parsing ligne {row.row_index}: {e}")
                        print(f"  raw_data: {row.row_data}")
                
                # VÃ©rifier la cohÃ©rence des donnÃ©es
                print(f"\nğŸ” ANALYSE DE COHÃ‰RENCE:")
                print(f"   - Nombre de lignes en base: {data_count}")
                print(f"   - Index min: {min(r.row_index for r in all_data) if all_data else 'N/A'}")
                print(f"   - Index max: {max(r.row_index for r in all_data) if all_data else 'N/A'}")
                
                # Analyser les colonnes
                if all_data:
                    first_row = json.loads(all_data[0].row_data)
                    columns = list(first_row.keys())
                    print(f"   - Colonnes trouvÃ©es: {columns}")
                    
                    # VÃ©rifier s'il y a des donnÃ©es vides
                    non_empty_rows = 0
                    for row in all_data:
                        data = json.loads(row.row_data)
                        if any(str(v).strip() for v in data.values() if v is not None):
                            non_empty_rows += 1
                    
                    print(f"   - Lignes avec donnÃ©es: {non_empty_rows}")
                    print(f"   - Lignes vides: {data_count - non_empty_rows}")
                
                # Sauvegarder les donnÃ©es pour inspection
                output_file = f"sql_dump_data_source_{source.id}.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    export_data = {
                        'source_info': {
                            'id': source.id,
                            'name': source.name,
                            'file_path': source.file_path,
                            'type': source.type,
                            'created_at': source.created_at.isoformat() if source.created_at else None,
                            'updated_at': source.updated_at.isoformat() if source.updated_at else None
                        },
                        'schema_info': json.loads(source.schema_info) if source.schema_info else None,
                        'data_count': data_count,
                        'rows': []
                    }
                    
                    for row in all_data:
                        try:
                            data = json.loads(row.row_data)
                            export_data['rows'].append({
                                'row_index': row.row_index,
                                'data': data
                            })
                        except json.JSONDecodeError:
                            export_data['rows'].append({
                                'row_index': row.row_index,
                                'data': None,
                                'raw_data': row.row_data
                            })
                    
                    json.dump(export_data, f, indent=2, ensure_ascii=False)
                
                print(f"ğŸ’¾ DonnÃ©es exportÃ©es dans: {output_file}")
        
        finally:
            db.close()
            
    except Exception as e:
        print(f"âŒ Erreur lors de l'inspection: {e}")
        import traceback
        traceback.print_exc()

def test_api_endpoint():
    """Test l'endpoint API pour voir ce qu'il retourne"""
    print(f"\nğŸ”— === TEST ENDPOINT API ===")
    
    try:
        import requests
        
        # Test avec l'endpoint public pour la source ID 1
        response = requests.get("http://localhost:8000/api/v1/preview/preview-data/1", timeout=10)
        
        print(f"ğŸ“¡ RÃ©ponse API pour source ID 1: {response.status_code}")
        if response.status_code == 200:
            data = response.json()
            print(f"âœ… DonnÃ©es rÃ©cupÃ©rÃ©es:")
            print(f"   - Lignes retournÃ©es: {len(data.get('rows', []))}")
            print(f"   - Total lignes: {data.get('total_rows', 'N/A')}")
            print(f"   - Source name: {data.get('data_source_name', 'N/A')}")
            
            if data.get('rows'):
                print(f"\nğŸ“‹ PREMIÃˆRES LIGNES RETOURNÃ‰ES:")
                for i, row in enumerate(data['rows'][:5]):  # 5 premiÃ¨res lignes
                    print(f"   Ligne {i+1}: {row}")
            else:
                print(f"âŒ Aucune ligne retournÃ©e par l'API!")
        else:
            print(f"âŒ Erreur API: {response.status_code}")
            print(f"   Response: {response.text}")
            
    except Exception as e:
        print(f"âŒ Erreur lors du test API: {e}")

def check_backend_processing():
    """VÃ©rifie le traitement backend des fichiers SQL"""
    print(f"\nğŸ”„ === VÃ‰RIFICATION TRAITEMENT BACKEND ===")
    
    # Chercher les fichiers SQL dans le rÃ©pertoire uploads
    upload_dir = Path("backend/uploads")
    if upload_dir.exists():
        sql_files = list(upload_dir.glob("*.sql")) + list(upload_dir.glob("*.dump"))
        if sql_files:
            print(f"ğŸ“ Fichiers SQL trouvÃ©s dans backend/uploads/:")
            for sql_file in sql_files:
                size = sql_file.stat().st_size
                print(f"   - {sql_file.name} ({size} bytes)")
        else:
            print(f"âŒ Aucun fichier SQL trouvÃ© dans backend/uploads/")
    else:
        print(f"âŒ RÃ©pertoire backend/uploads/ non trouvÃ©")

def main():
    """Fonction principale"""
    print("ğŸ” INSPECTION COMPLÃˆTE DES DONNÃ‰ES SQL DUMP")
    print("Ce script examine en dÃ©tail les donnÃ©es SQL dump stockÃ©es")
    print("=" * 70)
    
    # 1. Inspection base de donnÃ©es
    inspect_sql_dump_data()
    
    # 2. Test API
    test_api_endpoint()
    
    # 3. VÃ©rification fichiers
    check_backend_processing()
    
    print(f"\n" + "=" * 70)
    print("ğŸ INSPECTION TERMINÃ‰E")
    print(f"\nğŸ’¡ Analyse des rÃ©sultats:")
    print(f"   1. Si donnÃ©es en base = 0 â†’ ProblÃ¨me de synchronisation")
    print(f"   2. Si donnÃ©es en base > 0 mais API retourne 0 â†’ ProblÃ¨me endpoint")
    print(f"   3. Si API retourne des donnÃ©es mais tkinter n'affiche rien â†’ ProblÃ¨me interface")
    print(f"   4. Si donnÃ©es sont vides â†’ ProblÃ¨me de parsing SQL")

if __name__ == "__main__":
    main()